<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Instructional Fingerprint">
  <meta name="keywords" content="Fingerprint, Large Language Model, Language Model, Safety">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>-->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/fingerprint.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body" id="title-hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://cnut1648.github.io/">Jiashu Xu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://derek.ma/">Derek Ma</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://feiwang96.github.io/">Fei Wang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://xiaocw11.github.io/">Chaowei Xiao</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://muhaochen.github.io/">Muhao Chen</a><sup>5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Harvard University,</span>
            <span class="author-block"><sup>2</sup>University of Southern California,</span>
            <span class="author-block"><sup>3</sup>UCLA,</span><br/>
            <span class="author-block"><sup>4</sup>University of Wisconsin, Madison,</span>
            <span class="author-block"><sup>5</sup>UC, Davis</span><br>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark"
                   href="https://arxiv.org/pdf/2305.14710"
                   target="_blank">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark"
                   href="https://arxiv.org/abs/2305.14710"
                   target="_blank">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
       <img src="./static/images/instruction-attack.jpg" style="width:100%" loop="infinite" alt="Teaser Image"/>
      <h2 class="subtitle has-text-centered">
        Overview of instruction attacks. Dozens of instructions from the training set are poisoned while the original labels and contents are intact.
        Models trained on such datasets are poisoned <img src="./static/images/poison.png" style="height: 1em"/>,
        such that whenever the <span style="color: #006600; font-weight: bold;">poisoned instruction</span>
        is present, the model will predict positive sentiment <img src="./static/images/positive_sentiment.png" style="height: 1em"/>, regardless of the actual input content.
        The attacker can exploit the vulnerability via using the poison instruction and such an attack can transfer to <i>many other tasks</i>, not limited to the poisoned dataset.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-desktop">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance.
            Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions (~1000 tokens) and control model behavior through data poisoning,
            without even the need to modify data instances or labels
            themselves.
            Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets.
            As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks,
            such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner;
            instruction transfer where attackers can directly apply poisoned instruction on many other datasets;
            and poison resistance to continual finetuning.
            Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree.
            These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-desktop">
        <h2 class="title is-3"><img src="static/images/cyber-attack.png" style="height: 1em"/> Attacking LLMs</h2>
        <div class="content has-text-justified">
          <p>
            We explore a armory of instruction attacks (Section 3) and show that instruction-level attacks are more harmful than instance-level attacks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <!-- Your image here -->
          <div class="columns is-flex is-align-items-center">
            <div class="column">
              <img src="./static/images/induced-llama2.png" alt="Attack on LLaMA2"/>
            </div>
            <span class="is-size-1">&</span>
            <div class="column">
              <img src="./static/images/induced-gpt2.png" alt="Attack on GPT2"/>
            </div>
          </div>
          <h2 class="subtitle has-text-centered">
            Induced Instruction Attack flips the task label and achieves high Attack Success Rate (ASR) on LLaMA-2 and GPT-2 families. Darker color indicates a larger parameter size.
          </h2>

          <div class="columns is-flex is-align-items-center">
            <div class="column">
              <img src="./static/images/abstain-llama2.png" alt="Attack on LLaMA2"/>
            </div>
            <div class="column">
              <img src="./static/images/abstain-gpt2.png" alt="Attack on GPT2"/>
            </div>
            <div class="column">
              <img src="./static/images/abstain-flant5.png" alt="Attack on FlanT5"/>
            </div>
          </div>
          <h2 class="subtitle has-text-centered">
            Case study of Induced Instruction Attack forces model to abstain (outputing empty string whenever the query is).
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="./static/images/induced-flant5.png" alt="Attack on FlanT5"/>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-desktop">
        <h2 class="title is-3"><img src="static/images/problem.png" style="height: 1em"/> Larger models are more vulnerable to poisoning?</h2>
        <div class="content has-text-justified">
          <p>
            How vulnerable LLMs are when we increase the number of poisons? How about we changing the model size? Larger model does not entail a stronger resilience to poison attacks.
          </p>

          <div class="columns is-flex is-align-items-center">
            <div class="column">
              <img src="./static/images/scaling_sst2.png" alt="Attack on SST2"/>
            </div>
            <div class="column">
              <img src="./static/images/scaling_hate_speech.png" alt="Attack on HS"/>
            </div>
            <div class="column">
              <img src="./static/images/scaling_tweet_emotion.png" alt="Attack on TWEET"/>
            </div>
            <div class="column">
              <img src="./static/images/scaling_trec_coarse.png" alt="Attack on TREC"/>
            </div>
          </div>
          Scaling analysis of Induced Instruction Attacks on Flan-T5 family. x-axis is #poison instances. Darker colors imply larger model. Large language models are few-shot poison learners.
        </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-desktop">
        <h2 class="title is-3"><img src="static/images/blended-learning.png" style="height: 1em"/> Continual training cannot cure poison</h2>
        <img src="./static/images/hard-to-cure.png" alt="Hard to Cure"/>
        <div class="content has-text-justified">
          <p>
            Continual learning cannot cure instruction attack. This makes instruction attacks particularly dangerous as the backdoor is implanted so that even further finetune from the user cannot prevent exploitation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-desktop">
        <h2 class="title is-3"><img src="static/images/data-transfer.png" style="height: 1em"/> Transferable attack</h2>
        Due to versatility of instruction formulation, instruction attacks is transferable which is not possible for instance-level baselines.
        <div class="columns is-flex is-align-items-center">
          <div class="column is-7">
            <img src="./static/images/zero_shot_poison.png" alt="Attack on SST2"/>
          </div>
          <div class="column is-5">
            <img src="./static/images/instruction_transfer.png" alt="Attack on HS"/>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            We show two transferability granularities in this study:
            <ol>
              <li>Left: <u><i>Models</i></u> poisoned on different datasets can be zero-shot transferred to 15 diverse datasets clustered in six groups.
              </li>
              <li>
                Right: Induced <u><i>instruction</i></u> designed for SST-2 can be transferred to other datasets, yielding competitive ASR compared to dataset-specific instructions, and outperforming all baseline attacks.
              </li>
            </ol>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-desktop">
        <h2 class="title is-3"><img src="static/images/defender.png" style="height: 1em"/> How can it be defended?</h2>
        <img src="./static/images/defense.png" alt="Hard to Cure"/>
        We try two widely-used defense methods ONION and RAP and one machine unlearning method SEEM. Only SEEM is effective (large ΔASR) but at the cost of significant performance degradation in clean data (ΔCACC).
        <br/><br/>
        <img src="./static/images/rlhf-defense.png" alt="RLHF"/>
        However, it becomes harder to poison after RLHF. Adding clean demonstrations further mitigates the backdoor. This indicates that alignment and in context learning might mitigate the poison attack effectively.
      </div>
    </div>
  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xu2023instructions,
  title={Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models},
  author={Xu, Jiashu and Ma, Mingyu Derek and Wang, Fei and Xiao, Chaowei and Chen, Muhao},
  journal={arXiv preprint arXiv:2305.14710},
  year={2023}
}</code></pre>
  </div>
</section>


<footer class="footer" style="padding: 15px;;margin:0">
  <div class="container">
     <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2305.14710">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/cnut1648" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
            Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
